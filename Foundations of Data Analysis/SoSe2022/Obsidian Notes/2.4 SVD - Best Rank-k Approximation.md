%% #Lecture 5, 17.05. [[Foundations_L05_printout.pdf]] %%

## 2.4 Best Rank-$k$ Approximation
### Orthogonality of Singular Vectors

> **Lemma 5.1**
> The left singular vectors $u_1,\dots,u_r$ of a matrix $A$ are orthogonal.

###### #Proof Left SV are orthogonal
By induction: Assume $n-1$ SVs are orthogonal. Consider auxiliary matrix $B = A - \sigma u_1 v_1^H$. Now: the right SVs of $B$ are identical to the last $n-1$ right SVs of $A$, $v_2, \dots, v_n$. Furthermore, $B v_1 = A v_1 - \sigma_1 u_1 v_1^H v_1 = \sigma_1 u_1 - \sigma_1 u_1 = 0$.

Consider the leading SVec of $B$, $z$. ==Assume for contradiction== $z \not \bot v_1$. Project $z$ to $v_1$, $z_1 = \langle z, v_1 \rangle v_1$. Now $$||B \frac{z - z_1}{||z - z_1||_2} ||_2 = ||\frac{Bz - Bz_1}{||z - z_1||_2} ||_2.$$Because $B v_1 = 0$, $B z_1 = 0$. Hence $$... = \frac{||Bz||_2}{||z - z_1||_2}.$$We also know $||z - z_1||_2 < ||z||_2 = 1$ (why? #TODO). Therefore $$... > \frac{||Bz||_2}{||z||_2} = ||B z||_2$$which is a ==contradiction==, as $z$ should be the leading SVec.

Using the induction hypothesis on $B$, which has $n-1$ SVs, we get that these are orthogonal; we need to prove that $u_1 \bot u_j ~\forall j \geq 2$. ==Assume for contradiction== $\langle u_1, u_j \rangle \neq 0$, wlog. $\langle u_1, u_j \rangle > 0$. Consider a small $\epsilon  >0$ and define $$w := A\left( \frac{v_1 + \epsilon v_j}{||v_1 + \epsilon v_j||_2} \right) = \frac{A v_1 + \epsilon A v_j}{||v_1 + \epsilon v_j||_2} = \frac{\sigma_1 u_1 + \epsilon \sigma_j u_j}{\sqrt{1 + \epsilon^2}}$$
(in the last step we exploit orthogonality for the norm? But isn't that what we're trying to prove? Or do we already know it for the $v$-s, just not for the $u$-s? #TODO). $w$ has its length at least as long as its component along $u_1$: $$u_1^H \left( \frac{\sigma_1 u_1 + \epsilon \sigma_j u_j}{\sqrt{1+\epsilon^2}} \right) = \sigma_1 + \epsilon \sigma_j \langle u_1, u_j \rangle(1-\frac{\epsilon^2}{2} + O(\epsilon^4))$$(the last term uses a Taylor expansion of $1/\sqrt{1 + \epsilon^2}$ - the constant factor behaves as $\epsilon^2$ for small $\epsilon$). Furthermore,
$$\dots = \sigma_1 + \epsilon \sigma_j \langle u_1, u_j \rangle - O(\epsilon^2) > \sigma_1$$
for small enough $\epsilon$. This is a ==contradiction==, hence $u_1, u_j$ are orthogonal.

### $k$-Rank Truncations
Define the $k$-rank truncation of the SVD as 
$$A_k = \sum_{j=1}^k \sigma_j u_j v_j^H, \quad k \leq r.$$

> **Lemma 5.2**
> The rows of $A_k$ are *orthogonal projections* of the rows of $A$ onto the [[2.3 SVD - Further Singular Vectors#Best-Fit k -dimensional Subspace|best-fit k-dimensional subspace]] (which is spanned by the first $k$ SVecs of $A$)

###### #Proof Truncation <-> orthogonal projection
Assume $\mathbb K = \mathbb R$. We have $P_{V_k}(A^{(i)}) = \sum_l^{k} \langle A^{(i)}, v_l\rangle v_l$, recall that $\langle A^{(i)}, v_l \rangle = (A v_l)_i$. Also:
$$\sum_{l=1}^k A v_l v_l^\top = \sum_{l=1}^k \sigma_l u_l v_l^\top = A_k.$$ 
### Best $k$-Rank approximation in Frobenius Norm
> **Theorem  5.3**
> For any matrix $B$ of rank at most $k$,
> $$||A - A_k||_F \leq ||A - B||_F.$$

###### #Proof Frobenius-Norm Rank-$k$ Approximation
Assume $B$ is a rank $k$ (or less) matrix that minimizes $||A-B||_F$. Consider $V$ the space spanned by the rows of $B$, $\dim(V) \leq k$. Since $B$ minizes $||A - B||_F$, each row of $B$ must be the orthogonal projection of the corresponding rows in $A$ onto $V$ since
$$||A - B||_F^2 = \sum ||A^{(i)} - P_V(A^{(i)})||_2^2.$$
By Proposition 4.1 ( #TODO link), $V = V_k$. By Lemma 5.2 ( #TODO link), $B = A_k$.


%% #Lecture 6, 23.05. %%
### Best $k$-Rank approximation in Spectral Norm

> **Theorem 5.4**
> For any matrix $B$ of rank at most $k$,
> $$||A - A_k|| \leq ||A - B||$$

Towards a proof of Theorem 5.4, we compute the spectral norm approximation error of a $k$-Rank approximation as the $(k+1)$-th SV:

> **Lemma 5.5**
> The approximation error in spectral norm is given by $$||A - A_k||^2 = \sigma_{k+1}^2.$$

###### #Proof: Spectral norm approximation error of SVD (Lemma 5.5)
We have $A - A_k = \sum_{j={k+1}}^r \sigma_j u_j v_j^H$. Let $v$ be the top singular vector of $A - A_k$, represent it as $v = \sum_j \alpha_j v_j$ ( #TODO verify that $v \in span\{v_j | j\}$). Recall that the spectral norm is equal to the leading SV: [[2.3 SVD - Further Singular Vectors#SVs and the Spectral Norm]]

Then $||(A - A_k)v||_2 = ||(\sum_{j={k+1}}^r \sigma_j u_j v_j^H)(\sum_j \alpha_j v_j)||_2 = ||\sum_{j={k+1}}^r \alpha_j \sigma_j u_j||_2 = \sqrt{\sum_{j=k+1}^r \alpha_j^2 \sigma_j^2}$.

This quantity should be maximized under the contrainst $||v||_2 = \sum \alpha_j^2 = 1$. It's easy to see that the maximum is achieved for $\alpha_{k+1} = 1$, other $\alpha_j = 0$ as the $\sigma_j$ are ordered by size. Hence $||A-A_k||^2 = \sigma_{k+1}^2$.


###### #Proof $k$-Rank approximation minimizes spectral norm (Theorem 5.4)
Assume $rank(A) > k$ (otherwise $A_k = A$). Apply Lemma 5.5: $||A-A_k||^2 = \sigma_{k+1}^2$. Suppose there is a $B$ of rank at most $k$ s.t. $||A-B||^2 < \sigma_{k+1}^2$. Consider $\ker B$ which has dimension at least $n-k$. Let $\{v_i\}$ be the right singular vectors of $A$. By a dimension argument, $\Phi := \ker(B) \cap span\{v_1, \dots, v_{k+1}\} \neq \{0\}$.

That is: there exists $z \in \Phi$, $||z||=1$, s.t. $$||(A-B)z||_2^2 = ||Az||_2^2 = ||\sum_{j=1}^r \sigma_j u_j v_j^H z||_2^2 = \sum_{j=1}^r \sigma_j^2 |v_j^H z|^2$$
Since $z \in span\{v_1, \dots, v_{k+1}\}$, we can drop the summands for $j > k+1$ and get $$\dots = \sum_{j=1}^{k+1} \sigma_j^2 |v_j^H z|^2 \geq \sum_{j=1}^{k+1} \sigma_{k+1}^2|v_j^H z|^2 = \sigma_{k+1}^2 \sum_{j=1}^{n} |v_j^H z|^2$$
As the sum is equal to $||z||_2^2$ by the [[1 Basics#^b8bb8e|Pythagoras-Fourier Theorem]], we get $\dots = \sigma_{k+1}^2$.
So it follows that $||A-B||^2 = \sup_{||\hat z||=1}||(A-B)\hat z||_2^2 \geq ||(A-B)z||_2^2 \geq \sigma_{k+1} = ||A-A_k||^2$, yielding a contradiction. #TODO Verify this final argument.

