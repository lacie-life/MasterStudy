%% #Lecture 9, 13.06. [[Foundations Section 2.6 Applications.pdf]] %%

## 2.6 SVD Applications

### Applications
#### Application I: Customer Data
Assume $n$ customers have preferences for buying $d$ products: Matrix represents purchase probabilities (or the amount of utility of product $j$ to customer $i$)

![[customer-behavior.png]]

If the customer behavior is well described by only a small number of underlying factors, SVD allows to simplify the matrix and predict customer behavior. Then a customer's behavior can be approximately captured by a vector $(u_{\ell,i})_{\ell\in [k]}$ where $k \ll n, d$. Each *factor* $u_{\ell,i}$ comes with a vector $v_\ell$ that describes how this factor influences the purchasing probabilities.

Also related: matrix $A$ only partially known (a few entries); estimate a low-rank approximation given the few entries (needed e.g. in recommender systems).

#### Application II: Image Compression
Approximate image matrix via SVD. Example: middle picture is 50 components, right picture is 10 components.

![[svd-dogs.png]]

Another way to view it: *rank-1 matrices form a good representation system for images*, since only 50 are needed to decently represent this dog. The set of rank 1 matrices is still an infinite set; for some image classes, *finite* representation classes can be found.

#### Application III: Document Ranking
Represent documents as vectors that count word occurrences (say, the 25000 most important words in the English language). A collection of documents is then represented as a matrix. How to measure the intrinsic relevance of a document to the collection? Idea: project it to first SVec. 

Related: Website ranking, e.g. HITS and PageRank algorithms. Details see slides.


### Pseudo-Inverse Matrix
Given $A \in \mathbb K^{I \times J}$ of rank $r$ and its SVD, define its Moore-Penrose pseudo-inverse $A^\dagger$ as 
$$A^\dagger = \sum_{k=1}^r \sigma_k^{-1} v_k u_k^H = V \Sigma^{-1} U^H$$
(where in the matrix form, $\Sigma^{-1}$ means that the *non-zero* singular values are inverted).

If $A^H A \in \mathbb K^{J \times J}$ is invertible (i.e. $|I| \geq |J|$ and the columns are lin. independent), then $$A^\dagger = (A^H A)^{-1} A^H.$$
In this case, $A^\dagger$ is a left inverse of $A$: $A^\dagger A = I$. The matrix $A A^\dagger$ on the other hand corresponds to the orth. projection of $\mathbb K^I$ to the range (column span) of $A$.

Analogously if $A A^H \in \mathbb K^{J \times J}$ is invertible (i.e. $|I| \leq |J|$ and the rows are lin. independent), then
$$A^\dagger = A = A^H(AA^H)^{-1}.$$
In this case, $A^\dagger$ is a right inverse of $A$: $AA^\dagger = I$.

### Least Squares Problems
Two main cases:
- overdetermined systems: $Ax=y$ with more equations than unknowns. Might be not solvable: of particular interest are approximate solutions that minimize the mismatch $||Ax - y||_2$
- underdetermined systems: $Ax = y$ with fewer equations than unknowns. Has an infinite number of solutions. Of particular interest: solution that has minimal norm $||x||_2$.

> **Proposition 7.1**
> Among the $x$ that minimize $||Ax - y||_2$, the unique one with minimal Euclidean norm is $x^\ast = A^\dagger y$.

> **Corollary 7.2** (Overdetermined systems)
> Let $A \in \mathbb K^{I\times J}$ with $n = |I| \geq |J| = d$ have full rank $r = d$. The least squares problem $$\arg\min_{x \in \mathbb K^J} ||Ax - y||_2$$ has the unique solution $x = A^\dagger y$.

> **Corollary 7.3** (Underdetermined systems)
>  Let $A \in \mathbb K^{I\times J}$ with $n = |I| \leq |J| = d$ have full rank $r = n$. The least squares problem $$\arg\min_{x \in \mathbb K^J} ||Ax - y||_2 \quad \text{subj. to}~~ Ax=y$$ has the unique solution $x = A^\dagger y$.