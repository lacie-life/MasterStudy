%% #Lecture 4, 10.05. %%

## 2.3 Further Singular Vectors
### Best-Fit $k$-dimensional Subspace

> **Proposition**
> Let $v_1, \dots, v_r$ be the SVs, for each $k$, let $V_k = span\{v_1,\dots,v_k\}$. Then for each $k$, $V_k$ is the best-fit $k$-dim. subspace, i.e. $$V_k = \arg\min_{\dim V \leq k} \sum_i ||A^{(i)} - P_v A^{(i)}||_2^2$$$$\phantom{V_k} =\arg\max_{\dim V \leq k} \sum_i ||P_v A^{(i)}||_2^2$$

###### #Proof 
For simplicity assume $\mathbb K = \mathbb R$ (makes working with absolute values easier), $k=2$.
Let $V$ have orthogonal basis $\{\tilde v_j\}$.
$$\sum_{i,j} |\langle A^{(i)}, \tilde{v}_j \rangle|^2 = \sum_{i,j} |\langle A^{(i)}, \tilde v_j\rangle|^2 = \sum_i ||A\tilde v_j||_2^2 ~ \dots$$
We first showed the case $k=2$ and proceeded by induction. Induction is: $V_{k-1}$ best-fit $(k-1)$-dim. subspace => $V_k$ best-fit $k$-dim. subspace. We use a $W$ again and because of IH, $||Aw_1||_2^2 + \dots + ||Aw_{k-1}||_2^2 \leq ||Av_1||_2^2 + \dots + ||Av_{k-1}||_2^2$, exploiting $w_k \bot v_i \;\forall i$ one shows (how?) $||A w_k||_2^2 \leq ||A v_k||_2^2$.
#TODO maybe complete this proof at some point.

### The natural order of Singular Vectors
The singular vectors have corresponding *singular values* $\sigma_k = ||A v_k||_2$. The singular vectors come with a natural order: $\sigma_1(A) \geq \dots \geq \sigma_r(A)$.

The basis $v_1, \dots, v_r$ spans the row space of $A$. Therefore the number of singular vectors is equal to the rank of the matrix.

### SVs and Matrix norms
##### SVs and the Frobenius Norm
Consider one row $A^{(i)}$. $v_1, \dots, v_r$ span the rows of $A$, hence for any $v$ orthogonal to all $v_1, \dots, v_r$, $\langle A^{(i)}, v \rangle = 0$.

By Pythagoras: $||A^{(i)}||_2^2 = \sum_k |\langle A^{(i)}, v_k\rangle|^2$. Summing over all rows: $\sum_i ||A^{(i)}||_2^2 = \sum_{i,k} |\langle A^{(i)}, v_k \rangle|^2 = \sum_k ||A v_k||_2^2 = \sum_k \sigma_k(A)^2$. On the other hand, $\sum_i ||A^{(i)}||_2^2 = \sum_{i,j} |A_{ij}|^2 = ||A||_F^2$ is the squared Frobenius norm! In summary:

$$||A||_F = \sqrt{\sum_{k=1}^r \sigma_k(A)^2}$$
##### SVs and the Spectral Norm
The spectral norm is equal to the leading singular value:
$$\sigma_1(A) = \max_{||v||_2 = 1} ||Av||_2 = ||A||$$
##### SVs and the Schatten-$p$-Norms
For any $p \in [1, \infty)$, define the Schatten-$p$-Norm
$$||A||_p = \left( \sum_{k=1}^r \sigma_k(A)^p \right)^{1/p}$$

Schatten $p$-Norm has well-known special cases:
- $p=2$: Frobenius norm
- $p=\infty$: Spectral norm
- $p=1$: nuclear norm $||A||_\ast$ which is the sum of singular values

### Left and Right Singular Values
Define $u_i = \frac{A v_i}{\sigma_i (A)}$, the normalized versions of $Av_i$.
- $u_1, \dots, u_r$ are called *left singular vectors* of $A$
- $v_1, \dots, v_r$ are called *right singular vectors* of $A$

> **Lemma**
> The set of left singular vectors is orthnormal (as well).

> **Theorem** (Singular Value Decomposition)
> If $A \in \mathbb K^{I \times J}$ has right singular vectors $v_i$, left singular vectors $u_i$, and singular vectors $\sigma_i$, then $A$ can be written as
> $$A = \sum_{k=1}^r \sigma_k u_k v_k^H = U \Sigma V^H$$
> In other words: the left and right singular vectors and the singular values completely characterize the matrix.

![[svd-matrices.png]]


###### #Proof (SVD of a matrix)
Lemma we use for the proof: $A = B \Leftrightarrow \forall v: Av = Bv$.

For $v_j$ it holds: $A v_j = \theta_j u_j$. For any $v$, $v = \sum_{j=1}^r \langle v, v_j \rangle v_j + \sum_{j=r+1}^n \langle v, v_j^\bot \rangle v_j^\bot$. Next, $Av = A\left( \sum_{j=1}^r \alpha_j v_j + \sum_{j=r+1}^n \alpha_j v_j^\bot \right) = \sum_{j=1}^N \alpha_j A v_j + \sum_{j=N+1}^n \alpha_j A v_j^\bot$. For any $j = r+1,\dots,n$, $A v_j^\bot = 0$. So $Av = \sum \alpha_j A v_j = \sum_{i=1}^r \langle v, v_k \rangle A v_j = \sum_{j=1}^r \langle v, v_k \rangle \sigma_j u_j = \sum \sigma_j u_j v_k^H v = (\sum_{j=1}^r \sigma_j u_j v_j^H)v$. This concludes the proof.
