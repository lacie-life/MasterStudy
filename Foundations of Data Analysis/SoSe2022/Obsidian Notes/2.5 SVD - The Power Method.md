%% #Lecture 7, 30.05. [[Foundations Section 2.5 printout.pdf]] %%

## 2.5 - The Power Method
Assume for now that $A$ is real-valued, symmetric and square, has the same right and left SVecs and *has a spectral gap*: $\sigma_1 \gg \sigma_2$ (probably the absolute values, as well). In this case, the $\ell$-th power of $A$ is computed as
$$A^\ell = \sum_{k=1}^r \sigma_k^\ell v_k v_k^\top$$
If $\sigma_1 = 1$ (and hence the other coefficients smaller than 1), the powers of $A$ converge as follows: $A^\ell \to v_1 v_1^\top$. This allows to extract $v_1$. However, if $\sigma_1$ is known to be 1, we don't need the power method: we just have to solve the linear system $A v_1 = v_1$.

Instead:
- compute the norm $||A^m||_F$ (easily computed): For large $m$, it is dominated by $\sigma_1^m$.
- Thus $\frac{A^m}{||A^m||_F}$ converges to $v_1 v_1^\top$ as $m \to \infty$.

#### What if $A$ is not square?
Use $B = AA^\top$ instead: Then $B= \sum_{k=1}^r \sigma_k^2 u_k u_k^\top$ and $B^m = \sum_{k=1}^r \sigma_k^{2m} u_k u_k^\top$.

#### Further Issues
- what if there is no spectral gap? --> will be discussed later.
- computing $B^\ell$ costs $O(n^3 \log \ell)$ operations.
- a small spectral gap means a larger $\ell$ is needed, which makes the numbers in the matrix powers blow up beyond the floating point representable range.

```python
# assumes A to be real-valued, square + symmetric 
def power_method(A: np.ndarray, log_iterations: int = 10):
    for m in range(log_iterations):
        A = A @ A
    v1v1T = A / np.linalg.norm(A)
    v1 = v1v1T[:,0]
    return v1 / np.linalg.norm(v1)

# no assumptions on A
def randomized_power_method(A: np.ndarray, iterations: int):
    B = A @ A.conjugate().T
    x = random_unit_vector(B.shape[0])
    for m in range(iterations):
        x = B @ x
        x /= np.linalg.norm(x)
    return x
```

#### Reducing Complexity
Compute $A^\ell x$ instead of $A^\ell$ for some random vector $x$. Then
- only matrix-vector, not matrix-matrix products need to be computed
- if $x$ has a significant component in $u_1$ direction, after a few iterations this will be strongly dominant

If $x$ is chosen at random, $x_1 = \langle x, u_1 \rangle$ is *bounded away from zero with high probability*. Then $$A^l x \approx \sigma_1^{2k} u_1 (u_1^\top x).$$
This works because of the concentration of measure phenomenon.

### Concentration of Measure
Loosely speaking: *in high dimensions, a very large part of the mass of a ball is near the surface*.

> **Lemma 6.1**
> Let $x = (x_1, \dots, x_d)$ be a $d$-dimensional real random vector picked uniformly at random from the sphere (surface) $\{x : ||x||_2 = 1\}$. It holds that 
> $$|x_1| \geq \alpha > 0$$
> with probability at least $1 - C \alpha \sqrt{d}$ for some constant $C$ not depending on $\alpha$ or $d$.

^54a7e7

As a consequence, $x_1 = \langle x, u_1 \rangle$ of a unit random vector $x$ is bounded away from zero with "overwhelming probability", for any orthogonal basis $\{u_1, \dots, u_d\}$.

The result extends to complex random vectors for a different constant $C'$.

###### #Proof [[#^54a7e7|Lemma 6.1]]
Strategy: prove it for a $y$ picked uniformly at random from the unit ball (volume) $B(1)$. Then the projection $x=\frac{y}{||y||_2}$ is uniformly distributed on the sphere. As $||y||_2 \leq 1$, we have $|x_1| \geq |y_1|$ almost surely ( #TODO why only almost surely?). Hence $P(|x_1| \geq \alpha) \geq P(|y_1| \geq \alpha)$ and the result follows.

Proving the result for $y$ picked from the volume: 
$$\text{Vol}(B^d(1) \cap \{y : |y_1| \leq \alpha \})
= \int 1_{B^d(1) \cap \{y : |y_1| \leq \alpha \}}(y) \;dy
=\int_{-\alpha}^\alpha \int_{\mathbb R^{d-1}} 1_{y_2^2+\dots+y_d^2 \leq 1 - y_1^2}(y) \;dy_2,\dots,y_d\;dy_1
$$
$$
\dots = \int_{-\alpha}^{\alpha} \text{Vol}\left(B^{d-1}\left(\sqrt{1 - y_1^2}\right)\right) \;dy_1
= \text{Vol}(B^{d-1}(1)) \int_{-\alpha}^\alpha (1-y_1^2)^{\frac{d-1}{2}}\;d y_1
\leq 2\alpha \text{Vol}(B^{d-1}(1))
$$

%% #Lecture 8, 31.05. [[Foundations Section 2.5 printout.pdf]] %%

Recall: unit ball in $d$ dimensions' volume $V_1$ is asymptotically $\frac{1}{\sqrt{d\pi}} \left(\frac{2 \pi e}{d}\right)^{d/2}$. Hence, the probability we are looking for, $V_\alpha / V_1$ satisfies asympotically:

$$\frac{V_\alpha}{V_1} \leq \frac{2\alpha \text{Vol}(B^{d-1}(1))}{\frac{1}{\sqrt{d\pi}} \left(\frac{2 \pi e}{d}\right)^{d/2}}
= \frac{2\alpha \frac{1}{\sqrt{(d-1)\pi}} \left(\frac{2 \pi e}{(d-1)}\right)^{(d-1)/2}}{\frac{1}{\sqrt{d\pi}} \left(\frac{2 \pi e}{d}\right)^{d/2}}
= \frac{2\alpha}{\sqrt{2\pi e}}\sqrt{d}(1+\tfrac{1}{d-1})^\tfrac{d}{2}
$$
$$
\dots\leq \frac{2\alpha}{\sqrt{2\pi e}}\sqrt{d}(1+\tfrac{1}{d-1})^{\frac{d-1}{2}}
$$
...up to the factor $\sqrt{1+\tfrac{1}{d-1}}$ which is upper-bounded by $\sqrt{2}$ for $d \geq 2$. The last factor is upper-bounded by $\sqrt{e}$. Hence
$$... \leq \alpha \sqrt{\frac{2d}{\pi}}$$

Note: All of this only holds asymptotically; we might need another multiplicative constant to make it hold in general. Hence the constant $C$ in the theorem.

### The Randomized Power Method
The previous steps lead up to the following theorem:

> **Theorem 6.4** (Randomized power method)
> Let $A \in \mathbb K^{I \times J}$ and $x \in \mathbb K^I$ be a random unit vector. Let $V$ be the span of the *left singular vectors of $A$ corresponding to singular values greater than $(1-\epsilon)\sigma_1$*.
> Let $m \in \Omega\left(\frac{\ln(d/\epsilon)}{\epsilon}\right)$. Let $w^\ast$ be a unit vector after $m$ iterations of the power method: $$w^\ast = \frac{(AA^H)^m x}{||(A A^H)^m x||_2}$$
> Then $w^\ast$ has a component of at most $O\left(\frac{\epsilon}{\alpha d}\right)$ orthogonal to $V$ with probability at least $1-C\alpha \sqrt{d}$, for some constant $C$ not depending on $\alpha$ or $d$.

^d84598

###### #Proof [[#^d84598|Randomized Power Method]]
Let $A$ have the usual SVD $A = \sum \sigma_k u_k v_k^H$. If $r < n$, complete the $u_i$s to an orthonormal basis of the whole space. Write $x = \sum \langle x, u_k \rangle u_k$. 
Set $\sigma_k = 0$ for $k > r$: Then $(AA^H)^m = \sum_{k=1}^n \sigma_k^{2m} u_k u_k^H$. It follows that 
$$
(AA^H)^m x = \sum_{k=1}^m \sigma_k^{2m} u_k u_k^H x = \sum_{k=1}^m \sigma_k^{2m} u_k \langle x, u_k \rangle
$$
By [[#^54a7e7|Lemma 6.1]] (and its extension to $\mathbb C$), $|\langle x, u_1 \rangle | \geq \alpha > 0$ with probability at least $1 - C \alpha \sqrt{d}$.

Choose $r_\epsilon$ s.t. $\sigma_1, \dots, \sigma_{r_\epsilon}$ are the SV that are greater than $(1-\epsilon) \sigma_1$.

By the Pythagoras-Fourier Theorem:
$$||(AA^H)^m x ||_2^2 = \sum_{k=1}^n \sigma_k^{4m} |\langle x, u_k \rangle|^2 \geq \sigma_1^{4m} |\langle x, u_1 \rangle |^2 \geq \alpha^2 \sigma_1^{4m}$$
...with probability at least $1 - C\alpha \sqrt{d}$.

So the squared norm of the component of $(AA^H)^m x$ that is orthogonal to $V = span\{u_1,\dots,u_{r_\epsilon}\}$ is $\sum_{k=r_\epsilon + 1}^n \sigma_k^{4m} |\langle x, u_k \rangle|^2 \leq (1-\epsilon)^{4m} \sigma_1^{4m} \sum_{k=r_\epsilon + 1}^n |\langle x, u_k\rangle|^2 \leq (1-\epsilon)^{4m} \sigma_1^{4m}$, as $\sum_{k=r_\epsilon + 1}^n \leq ||x||_2^2 = 1$.

Thus, the component of $w^\ast$ orthogonal to $V$ satisfies:
$$||P_{V^\bot} w^\ast||_2^2
= \frac{\sum_{k=r_\epsilon + 1}^n \sigma_k^{4m} |\langle x, u_k \rangle|^2}{\sum_{k=1}^n \sigma_k^{4m} |\langle x, u_k \rangle|^2}
\leq \frac{(1-\epsilon)^{4m} \sigma_1^{4m}}{\alpha^2 \sigma_1^{4m}}
= \frac{(1-\epsilon)^{4m}}{\alpha^2}
$$

Idea: $1-\epsilon$ is a linear approximation of $\exp(-\epsilon)$. Hence $(1-\epsilon)^{2m}$ approximates $\exp(-2m)$ for small $\epsilon$. Thus the component of $w^\ast$ orthogonal to $V$ has norm at most $$\frac{(1-\epsilon)^{2n}}{\alpha} = O\left(\alpha^{-1} e^{-2\epsilon m}\right)
= O\left(\alpha^{-1} e^{-2 \Omega (\ln(d/\epsilon))}\right)
= O\left(\frac{\epsilon}{\alpha d}\right).$$
where in the last step, a constant in $m$ needs to be substituted (How? #TODO).


##### Intuitively/Concretely
Let's take $\alpha = 1/(10 C d)$. Then with probability $1 - \frac{1}{10 \sqrt{d}}$, i.e. almost 1, after $m$ iterations, $w^\ast$ has a component of at most $O(\epsilon)$ orthogonal to $V$.

Choosing $\epsilon$: If we choose $\epsilon = 1$, $V$ is the whole space spanned by the matrix and the statement is probably more or less trivial (at least in the case of a full-rank matrix, it's obvious: There cannot be a part in any orthogonal direction to the full space). If $\epsilon$ is chosen to be small, $V$ will be spanned only by the first SVec and other SVecs that are almost equally important. If $\epsilon$ is small enough and there is a unique leading SVec, $V$ will be spanned by the leading singular vector and component of $w^\ast$ orthogonal to $V$ will be an unwanted component distracting from the leading SVec. The theorem guarantees that, as $\epsilon$ is small, this distracting component will be small proportional to $\epsilon$ with probability close to 1.