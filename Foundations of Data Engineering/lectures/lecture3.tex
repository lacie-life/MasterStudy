\section{SQL}
SQL is a declarative language (orders to execute a query without specifying how) to interact with databases and retrieve data. Optimization is not only on the query level, but also on storage and data types: for instance, strings can be managed adding a pointer to their size (if they are long). 

Various SQL dialects have slight differences between semantics, but there are general standards starting from older versions. Different databases might also have different type management, operations and speed.

Another common problem with databases is handling null values while evaluating boolean expressions: NULL and FALSE results in FALSE, therefore this may give an inaccurate representation of information. On the other hand, NOT NULL columns are beneficial for optimization (NULL columns have a reserved memory space for this possibility).

Some PostgreSQL data types:
\begin{itemize}
	\item Numeric, slow arbitrary precision numbers with unbounded size;
	\item Float, mobile floating point (4 bytes) or double precision (8 bytes);
	\item Strings of variable length (pre-allocated or dynamic);
	\item Other common types such as \texttt{bytea}, \texttt{timestamp} and \texttt{interval}.
\end{itemize} 

\subsection{SQL commands}
Standard SQL commands are for instance SELECT, FROM, WHERE, GROUP BY, ORDER BY. Those can be found in normal queries or subqueries, which allow to temporarily subset data without saving it to memory.

The \texttt{\textbackslash copy} statement allows to import data from a text file, but can be slow with large datasets since it requires a full scan.

Pattern matching can be done either by comparison (=) or regular expression (LIKE or $\sim$).

Random samples can be extracted with various methods, of which the most accurate in randomness (Bernoulli) is also the slowest.

Set operations are performed thanks to the following instructions:
\begin{enumerate}
	\item UNION, which vertically merges two tables with the same attributes;
	\item EXCEPT, which performs the set difference \texttt{A - B};
	\item INTERSECT, to only keep rows in common between two tables.
\end{enumerate}
The default option implies removing duplicates; to keep them, specifying ALL is necessary.

\subsection{Views and CTEs}
A view is an abstract database object of a stored query, accessed as a virtual table but physically not containing data. It simplifies complexity of interrogations because it wraps commonly used information and makes it reusable.

Views are faster, but can be viewed globally, causing potential issues with naming. 

A better way to optimize through a temporary table is \texttt{WITH}, a statement which defines common table expressions (CTE).

CTEs are temporary result sets that can be referenced with another SQL instruction to simplify and speed up queries. Results get materialized and then scanned. The main difference from views is that CTEs have a scope only within the query, and get destroyed as soon as the execution is finished. 

CTEs overcome SELECT statement limitations such as referencing themselves, which is the basis of recursion. 

\subsection{Regular expression matching}
There are different algorithms which establish whether a given regex matches a string:
\begin{enumerate}
	\item Deterministic finite automata, built from a non-deterministic one, which allows to match in linear time yet is constructed in exponential state (to the size);
	\item NFA with caching to simulate a DFA, avoiding the construction cost but raising the computational to almost quadratic;
	\item Pattern matching by backtracking, forcing an exponential number of sub-cases.
\end{enumerate}
Variations of the Boyer-Moore algorithm (grep) are commonly used, optimized with automata. 

Postgres uses Harry Spencer's \texttt{regex} library for matching, an hybrid DFA/NFA implementation (thank you Tom Lane for making clear documentation) which initially generates a NFA representation, then optimizes it and executes a DFA materializing states only when needed and keeping them in cache. 

The parser also includes an NFA mode to implement features such as capturing parentheses and back-references. It constructs a tree of sub-expressions which is recursively scanned similarly to tradition engines, therefore quite slow. 

To make this faster, each node has an associated DFA representing what it could potentially match as a regular expression. Before searching for sub-matches, the string goes through the DFA so that if it does not match the following stages can be avoided.

\subsection{String comparison}
String comparison is a task which can be computationally expensive in case of very large output. 

There are several methods to implement this, such as suffix trees and edit distance, which also allow to search for errors in text through string similarity. 

The metric represents the number of operations that need to be applied to one of the inputs to make it equal to the other one, through dynamic programming (construction of a matrix). 

\subsection{Indexing}
Postgres provides several index types, of which the most common is the B-tree, a self-balancing data structure allowing logarithmic-time operations (the most important is searching).

It is useful because nodes can have more than two children: the number of sub-trees can in fact be a multiple of the file system block size. The structure is optimized to have few accesses to disk avoiding loading data within reads.

Further improvements can be made creating auxiliary indexes containing the first record in each disk block, narrowing the search in the main database.

\subsection{Counting}
Counting, unless an index is present, is not optimizable: to have an exact result, the DBMS must perform a linear scan of the table, to avoid eventual transaction conflicts.

Counting distinct values or grouping before joining, on the other hand, is faster since it involves an approximation.

Number of total rows can be approximated as well, thanks to sketches: probabilistic data structures that contain frequency tables of events in a stream of data.

They use hash tables to map events to frequencies, and at any time the sketch can be queried for the frequency of a particular element. 

To reduce variance and have a more accurate estimation, many independent copies can be stored in parallel, and their outputs is combined.

\subsection{Count-distinct problem}
The count-distinct problem finds how many unique elements are there in a dataset with duplicates. 

The naive implementation consists in initializing an efficient data structure (hash table or tree) in which insertion and membership can be performed quickly, but this solution does not scale well.

Streaming algorithms use randomization to produce a close approximation of the number, hashing every element and store either the maximum (likelihood estimator) or the $m$ minimal values.

\subsubsection{HyperLogLog}
HyperLogLog is one of the most widely used algorithms to count distinct elements avoiding calculating the cardinality of a set.

It is a probabilistic algorithm, able to estimate cardinalities of $10^{9}$ with a typical standard error around 2\% using very little memory. 

The process derives from Flajolet-Martin algorithm: given a hash function which uniformly distributes output, it is taken the least-significant set bit position from every binary representation. 

Since data is uniformly distributed, the probability of having an output ending with $2^k$ (one followed by $k$ zeros) is $2^{-(k+1)}$. The cardinality is then estimated using the maximum number of leading zeros. With $n$ zeros, the number of distinct elements is circa $2^n$.

HyperLogLog also allows to add new elements, count and merge to obtain the union of two sets.

\subsection{Sampling}
Sampling is usually performed through the hypergeometrical distribution, a formula composed by 3 binomial coefficients. Computing factorials, approximated with exponentiation, can be a heavy operation.

Factorials can be estimated using Stirling approximation: $\log(n!) \approx n\log n - n + \frac{1}{2}(2\pi n)$, which is cheaper and works for sufficiently big $n$, but still not efficient for very large numbers.

\section{Query decorrelation}
Often queries are easier to formulate using subqueries, yet this can take a toll on performance: since SQL is a declarative language, the best evaluation strategy is up to the DBMS.

A subquery is correlated if it refers to tuples from the outer query. The execution of a correlated query is poor, since the inner one gets computed for every tuple of the outer one, causing a quadratic running time.

Correlation can be recognized observing fields that do not have a bound with the attributes of the outer query. In SQL, they usually are in WHERE or EXISTS statements.

Example (from TCP-H dataset):
\begin{lstlisting}[language=SQL]
select avg(l_extendedprice)
from lineitem l1
where l_extendedprice =
	(select min(l_extendedprice)
	from lineitem l2
	where l1.l_orderkey = l2.l_orderkey;)
\end{lstlisting}

This query can be rewritten to avoid correlation:
\begin{lstlisting}[language=SQL]
select avg(l_extendedprice)
from lineitem l1,
	(select min(l_extendedprice) m, l_orderkey
	from lineitem
	group by l_orderkey) l2
where l1.l_orderkey = l2.l_orderkey
and l_extendedprice = m;
\end{lstlisting}

The new query is much more efficient, yet not as intuitive. Compilers sometimes manage to automatically decorrelate, but correlations can be complex (inequalities, disjunctions).

Instead of running a function every time, the subset is pre-computated trying to evaluate all the possible choices to then just use a lookup.

A join dependency allows to recreate a table by joining other tables which have subsets of the attributes (relationships are independent of each other). In other words, a table subject to join dependency can be decomposed in other tables and created again joining them.

Dependent joins rely on nested loop evaluations, which are very expensive: the goal of unnesting is to eliminate them all.

The easiest practice consists in pulling predicates up and creating temporary tables inside FROM instead of WHERE, evaluating the subquery for all possible bindings simultaneously. 

The subquery in some cases is a set without duplicates which allows equivalence between dependent join and bindings of free variables, therefore dependency can be removed.

Sometimes attributes of the set can be inferred, computing a superset of the values and eliminating them with the final join. This avoids computing the subquery, but causes a potential loss of pruning the evaluation tree.

The approach is overall always working with relational algebra, yet not necessarily on the SQL level: it can add memory overhead which cannot always be removed, despite the ideal linear computational time.

To decorrelate subqueries, a general method can be used which is effective even with more complicated interrogations containing inequalities and several conditions.

The procedure can be executed several times, always starting from the highest level of nesting:
\begin{enumerate}
	\item Moving the correlated subquery to FROM;
	\item Since there might be multiple keys with the same value, putting DISTINCT helps reducing cardinality;
	\item Joining is performed in the outer query, and if there is more than one value, an OR condition is applied so that either can be possible;
	\item Values are grouped to maintain the same domain, according to the elements in DISTINCT and JOIN;
	\item Other conditions taken from outside the subquery are applied after pre-aggregation.
\end{enumerate}

The inner table should always have an unique keys and some groups, which then can be selected in the outer one.

Example:
\begin{lstlisting}
select o_orderpriority ,count (*) as order_count
from orders
where o_orderdate >= date '1993-07-01'
and o_orderdate < date '1993-10-01'
and exists (
	select * 
	from lineitem
	where l_orderkey = o_orderkey
	and l_commitdate < l_receiptdate)
group by o_orderpriority
order by o_orderpriority;
\end{lstlisting}

Decorrelated query:
\begin{lstlisting}[language=SQL]
select o_orderpriority, count(*) as order_count
from orders, (
	select o2.o_orderkey
	from lineitem, (
		select distinct o_orderkey 
		from orders) o2
	where l_orderkey = o2.o_orderkey
	and l_commitdate < l_receiptdate
	group by o2.o_orderkey) preagg
where o_orderdate >= date '1993 -07 -01' 
and o_orderdate < date '1993 -10 -01' 
and orders.o_orderkey = preagg.o_orderkey
group by  o_orderpriority
order by  o_orderpriority;
\end{lstlisting}
